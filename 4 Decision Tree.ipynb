{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "❗️ **If there is a problem rendering this article (garbled or incorrect format), please feel free to visit my Github repository for this course:**\n",
    "\n",
    "https://github.com/jimcui3/Introduction-to-Machine-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *Introduction to Machine Learning* Algorithms and Realizations 4\n",
    "\n",
    "## By Jiaheng Cui\n",
    ">In this chapter, we'll talk about Decision Trees, a widely-used model in supervised learning. We'll introduce three basic algorithms to generate a decision tree: ID3, C4.5, CART. Meanwhile, we'll introduce prunning techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Decision Trees\n",
    "### (1) Introduction\n",
    "\n",
    "### (2) Feature selection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.ID3 Algorithm\n",
    "### (1) Entropy and Information gain\n",
    "\n",
    "### (2) ID3 Algorithm\n",
    "Input: training instances $X = \\left[\\begin{matrix}  x_{11} & \\cdots & x_{1p}\\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} & \\cdots & x_{np} \\\\ \\end{matrix}\\right]$, label vector $y = (y_1, y_2, ..., y_n)^T$.\n",
    "\n",
    "Output: parameter vector $\\hat{\\beta}$\n",
    "\n",
    "Step1: load $X$ and $y$, note that there isn't a \"1\" in the front of each row, we'll add them manually. \n",
    "\n",
    "Step2: let $\\hat{\\beta} = (X^T X)^{-1}X^T y$, so the model we trained is $y = X \\hat{\\beta}$.\n",
    "\n",
    "### (3) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from numpy import log2 as log # \"log\" is the logarithm to base 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ID3_Classification_Tree():\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.label = list(df)\n",
    "     \n",
    "    \n",
    "    # Calculate the entropy of different attributes\n",
    "    def find_entropy(self, df):\n",
    "        entropy = 0\n",
    "        Class = df.keys()[-1] # self.df.keys()[-1] is the last column, i.e. the label\n",
    "        values = df[Class].unique()# We want to devide df by the label, so we find the distinct labels\n",
    "        \n",
    "        for value in values:\n",
    "            fraction = df[Class].value_counts()[value]/len(df[Class])# fraction_i = |C_k|/|D|\n",
    "            \n",
    "            if(fraction != 0): # Let 0*log0 = 0, otherwise we cannot compute the entropy\n",
    "                entropy += -fraction * log(fraction)# entropy_i = -|C_k|/|D|* log(|C_k|/|D|)\n",
    "                \n",
    "        return entropy\n",
    "\n",
    "    \n",
    "    # Calculate the entropy within an attribute (i.e. subsets)\n",
    "    def find_entropy_attribute(self, df, attribute):\n",
    "        Class = df.keys()[-1]\n",
    "        target_variables = df[Class].unique()\n",
    "        variables = df[attribute].unique() # Find the distinct features within the attribute\n",
    "        conditional_entropy = 0\n",
    "        \n",
    "        for variable in variables:\n",
    "            entropy = 0\n",
    "            \n",
    "            for target_variable in target_variables:\n",
    "                num = len(df[attribute][df[attribute] == variable][df[Class] == target_variable]) # |D_ij|\n",
    "                den = len(df[attribute][df[attribute] == variable]) # |D_i|\n",
    "                fraction = num/den # |D_ij|/|D_i|\n",
    "                \n",
    "                if(fraction != 0):\n",
    "                    entropy += -fraction * log(fraction) # entropy is the entropy of one of the values in the attribute\n",
    "                    \n",
    "            fraction2 = den/len(df) # |D_i|/|D|\n",
    "            conditional_entropy += fraction2 * entropy # conditional_entropy is the conditional entropy under the attribute\n",
    "            \n",
    "        return conditional_entropy\n",
    "    \n",
    "    \n",
    "    # Find the attribute that maximizes the information gain\n",
    "    def find_winner(self, df): \n",
    "        competitors = [] # competitors is all possible moves of an node\n",
    "        \n",
    "        for key in df.keys()[:-1]: # df.keys()[:-1] is all the attributes of df except for the label\n",
    "            competitors.append(self.find_entropy(df) - self.find_entropy_attribute(df, key)) # Compute the information gain of key\n",
    "            \n",
    "        return df.keys()[:-1][np.argmax(competitors)] \n",
    "\n",
    "    \n",
    "    # Get the subset of a node by its value\n",
    "    def get_subtable(self, df, node, value):\n",
    "        return df[df[node] == value].reset_index(drop = True)\n",
    "\n",
    "    \n",
    "    def buildTree(self, df): \n",
    "        Class = df.keys()[-1]\n",
    "        node = self.find_winner(df) # Find the attribute that maximizes the information gain\n",
    "        attValue = np.unique(df[node]) # Get distinct value of the attribute\n",
    "        tree = {}\n",
    "        tree[node] = {}\n",
    "\n",
    "        for value in attValue:\n",
    "            subtable = self.get_subtable(df, node,value)\n",
    "            clValue, counts = np.unique(subtable[subtable.keys()[-1]], return_counts = True)\n",
    "\n",
    "            if (len(counts) == 1):# Subset is pure, stop generating the tree from this path\n",
    "                tree[node][value] = clValue[0]\n",
    "                \n",
    "            else:        \n",
    "                tree[node][value] = self.buildTree(subtable) # Subset is not pure, so call the function recursively\n",
    "                \n",
    "        return tree\n",
    "    \n",
    "    \n",
    "    def train(self):\n",
    "        self.tree = self.buildTree(self.df)\n",
    "    \n",
    "    \n",
    "    def predict_iteration(self, tree_dict, test_column):\n",
    "        firstStr = list(tree_dict.keys())[0] # Find the attribute of the first node\n",
    "        secondDict = tree_dict[firstStr] # Find the subtree of this node\n",
    "        featureIndex = self.label.index(firstStr) # Find the subset of that subtree\n",
    "\n",
    "        for key in secondDict.keys():\n",
    "            if(test_column[featureIndex] == key):\n",
    "                if(isinstance(secondDict[key], dict)): # If this node is a dict, this node is a inner node, so call the function recursively.\n",
    "                    classlabel = self.predict_iteration(secondDict[key], test_column)\n",
    "\n",
    "                else: # This node is a leaf, just output the result.\n",
    "                    classlabel = secondDict[key]\n",
    "                    \n",
    "        return classlabel\n",
    "    \n",
    "    \n",
    "    def predict(self, tree_dict, test_data):\n",
    "        predicted_labels = list()\n",
    "        \n",
    "        for i in range(test_data.shape[0]):\n",
    "            predicted_labels.append(self.predict_iteration(tree_dict, test_data[i,:])) # Evaluate each line.\n",
    "            \n",
    "        return predicted_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the dataset which we'll use to train the model. We put it into a dataframe named `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = np.array([['较高','是','是','否','感冒'],\n",
    "                         ['非常高','否','否','否','不感冒'],\n",
    "                         ['非常高','是','否','是','感冒'],\n",
    "                         ['正常','是','是','是','感冒'],\n",
    "                         ['正常','否','否','是','不感冒'],\n",
    "                         ['较高','是','否','否','感冒'],\n",
    "                         ['较高','是','否','是','感冒'],\n",
    "                         ['非常高','是','是','否','感冒'],\n",
    "                         ['较高','否','是','是','感冒'],\n",
    "                         ['正常','是','否','否','不感冒'],\n",
    "                         ['正常','是','否','是','感冒'],\n",
    "                         ['正常','否','是','是','感冒'],\n",
    "                         ['较高','否','否','否','不感冒'],\n",
    "                         ['非常高','否','是','否','感冒'],\n",
    "                         ['非常高','否','是','否','感冒'],\n",
    "                         ['较高','否','否','是','感冒']])\n",
    "\n",
    "labels = np.array(['体温','流鼻涕','肌肉疼','头疼','感冒'])\n",
    "\n",
    "df = pd.DataFrame(training_set, columns = labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train an ID3 classification tree model `decision_tree`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "decision_tree = ID3_Classification_Tree(df)\n",
    "decision_tree.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the decision tree by calling `print(decision_tree.tree)`, the result will be a dict:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'肌肉疼': {'否': {'流鼻涕': {'否': {'体温': {'正常': '不感冒', '较高': {'头疼': {'否': '不感冒', '是': '感冒'}}, '非常高': '不感冒'}}, '是': {'体温': {'正常': {'头疼': {'否': '不感冒', '是': '感冒'}}, '较高': '感冒', '非常高': '感冒'}}}}, '是': '感冒'}}\n"
     ]
    }
   ],
   "source": [
    "t = decision_tree.tree\n",
    "print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: we can use the python package **graphviz** to visualize a decision tree and output a pdf file. Those who are interested can find it here:\n",
    "\n",
    "https://github.com/xflr6/graphviz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can predict new data by calling `decision_tree.predict(t, test_set)`, where t is the tree, and test_set is the new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['不感冒', '感冒']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set = np.array([['较高','否','否','否'], ['非常高','否','是','否']])\n",
    "decision_tree.predict(t, test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.C4.5 Algorithm\n",
    "### (1) Information gain ratio\n",
    "\n",
    "### (2) Basic thoughts of C4.5 algorithm\n",
    "\n",
    "### (3) PEP Post-prunning\n",
    "\n",
    "### (4) Algorithm\n",
    "Input: training instances $X = \\left[\\begin{matrix}  x_{11} & \\cdots & x_{1p}\\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} & \\cdots & x_{np} \\\\ \\end{matrix}\\right]$, label vector $y = (y_1, y_2, ..., y_n)^T$.\n",
    "\n",
    "Output: parameter vector $\\hat{\\beta}$\n",
    "\n",
    "Step1: load $X$ and $y$, note that there isn't a \"1\" in the front of each row, we'll add them manually. \n",
    "\n",
    "Step2: let $\\hat{\\beta} = (X^T X)^{-1}X^T y$, so the model we trained is $y = X \\hat{\\beta}$.\n",
    "\n",
    "### (5) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.CART Algorithm\n",
    "### (1) Gini index\n",
    "\n",
    "### (2) CART classification tree\n",
    "\n",
    "### (3) CART regression tree\n",
    "\n",
    "### (4) CCP Post-prunning\n",
    "\n",
    "### (5) Algorithm\n",
    "Input: training instances $X = \\left[\\begin{matrix}  x_{11} & \\cdots & x_{1p}\\\\ \\vdots & \\ddots & \\vdots \\\\ x_{n1} & \\cdots & x_{np} \\\\ \\end{matrix}\\right]$, label vector $y = (y_1, y_2, ..., y_n)^T$.\n",
    "\n",
    "Output: parameter vector $\\hat{\\beta}$\n",
    "\n",
    "Step1: load $X$ and $y$, note that there isn't a \"1\" in the front of each row, we'll add them manually. \n",
    "\n",
    "Step2: let $\\hat{\\beta} = (X^T X)^{-1}X^T y$, so the model we trained is $y = X \\hat{\\beta}$.\n",
    "\n",
    "### (6) Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use decision tree algorithms from **sklearn**, please note that they only accept numerical values, but not categorical values. So you need to first transform the categorical values into numerical values using `preprocessing.LabelEncoder()`.\n",
    "\n",
    "References: \n",
    "1. Decision tree from sklearn: https://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "2. LabelEncoder from sklearn: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n",
    "\n",
    "3. Using LabelEncoder to transform categorical variables: https://stackoverflow.com/questions/38108832/passing-categorical-data-to-sklearn-decision-tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "1.机器学习 - 周志华\n",
    "\n",
    "2.Hands-on Machine Learning with Scikit-Learn, Keras & TensorFlow, 2nd Edition - Aurélien Géron\n",
    "\n",
    "3.统计学习方法（第2版）- 李航\n",
    "\n",
    "4.ID3 algorithm - Wikipedia\n",
    "\n",
    "5.数据挖掘十大算法（一）：决策树算法 python和sklearn实现 - CSDN\n",
    "\n",
    "https://blog.csdn.net/qq_36523839/article/details/81408326\n",
    "\n",
    "6.Decision Trees from Scratch Using ID3 Python: Coding It Up !!\n",
    "\n",
    "https://medium.com/@lope.ai/decision-trees-from-scratch-using-id3-python-coding-it-up-6b79e3458de4\n",
    "\n",
    "7.python：从零散的字典组装成树状嵌套字典 - CSDN\n",
    "\n",
    "https://blog.csdn.net/qq_17065591/article/details/107528137\n",
    "\n",
    "8.Python嵌套字典的遍历 - CSDN\n",
    "\n",
    "https://blog.csdn.net/Tw_light/article/details/104961524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
